{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fairface-VGG-experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "U6UxSpvNNyOp",
        "fKGkVc0wiZzR",
        "LEBDdKt6i_ed",
        "AIN37LPApf_o",
        "NNxphiFawFCQ",
        "TZTmITAyRTIB",
        "PKH3xz6psDPU",
        "Gq_OSCx19xur",
        "zuVD6A-MfMhW"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qianqiancui/Pisturk-Trait-Learning-Task/blob/master/notebooks/fairface_VGG_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y7mt_du6-GR"
      },
      "source": [
        "# Race prediction from face images using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwEi5WszAVun"
      },
      "source": [
        "\n",
        "\n",
        "* Gender : `Male` or `Female`\n",
        "* Race : based on the labels in the datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zcLiRsp6-GU"
      },
      "source": [
        "## 1. Prepare\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86lX5nQdLG9J"
      },
      "source": [
        "Before you run the note book, click **runtime**, select **change runtime type**, choose **GPU**. Then click **run all** or run sections manually based on your need.\n",
        "\n",
        "I strongly suggest that you create a copy of the notebook each time you manipulate the # of images for black and white faces or make big changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb_-mrdtKPuL"
      },
      "source": [
        "### load resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X1Mgstb2YrH"
      },
      "source": [
        "# link google drive to colab, so that we are able to load drive files into the colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruVsrDizU0jo",
        "outputId": "71b2a3cc-759d-4898-d387-f5d1aa106b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# load repository to colab \"dl_repo\" is the name of the repo\n",
        "! pip install /content/gdrive/My\\ Drive/dl_repo\n",
        "\n",
        "# import python modules \n",
        "import torch\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# import our own modules included in the scripts\n",
        "import torchvision.transforms as transforms\n",
        "from vision_utils.custom_torch_utils import load_model\n",
        "from vision_utils.custom_architectures import SepConvModelMT, SepConvModel, initialize_model, PretrainedMT\n",
        "from multitask_rag.train import run_utk\n",
        "from multitask_rag.utk_data_utils import get_utk_dataloader, split_utk\n",
        "from multitask_rag.evaluate import evaluate_model as eval_utk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./gdrive/My Drive/dl_repo\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from democlassi==0.5) (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from democlassi==0.5) (0.7.0+cu101)\n",
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/98/0a5b83d82ff245d3de5f09808fb80ff0ed03f6b10933979e6018b1dd0eaa/pytorch_ignite-0.4.2-py2.py3-none-any.whl (175kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from democlassi==0.5) (4.1.2.30)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from democlassi==0.5) (7.0.0)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (from democlassi==0.5) (0.5.3)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->democlassi==0.5) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->democlassi==0.5) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->democlassi==0.5) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->democlassi==0.5) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->democlassi==0.5) (50.3.0)\n",
            "Building wheels for collected packages: democlassi\n",
            "  Building wheel for democlassi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for democlassi: filename=democlassi-0.5-cp36-none-any.whl size=10069745 sha256=7f6d98b7c308896d40eccbaf1e5fd6842050f1b9b733abd6c0ff68d12144b949\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-95hp8tvk/wheels/3b/65/f2/004ee8c3b812c6b226984d1dadd6248e7732b15ea4422ca91d\n",
            "Successfully built democlassi\n",
            "Installing collected packages: pytorch-ignite, tensorboardX, democlassi\n",
            "Successfully installed democlassi-0.5 pytorch-ignite-0.4.2 tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkFX8o4zB-en"
      },
      "source": [
        "# Create a directory where to store data\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "\n",
        "# load the dataset and unzip \n",
        "!cp /content/gdrive/My\\ Drive/ff.zip /content/data/\n",
        "!unzip data/ff.zip -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2czpStHRsNOo"
      },
      "source": [
        "# print content of the data directory\n",
        "!ls /content/data/\n",
        "\n",
        "# check # of images\n",
        "list_images = glob.glob('/content/data/ff/*jp*')\n",
        "print('total images:',len(list_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_C4M_JG6wO"
      },
      "source": [
        "We have 86744+10954 images in total. The image names format is the following : `age_gender_race_date`.\n",
        "for instance this image name `1_0_0_20161219140623097.jpg.chip.jpg` suggests: age is `1`, gender is `0` (Male) and race is `0` (White).\n",
        "However there are few images for which the name is malfomed, so we remove them using the following code snippet :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQPpWkkvseJV"
      },
      "source": [
        "# function to remove invalid images (that he filenames is not correctly formatted)\n",
        "def get_invalid_images(root_path='/content/data/ff/'):\n",
        "    list_files = glob.glob(os.path.join(root_path, '*.[jJ][pP]*'))\n",
        "    filenames = [path.split('/')[-1].split('_') for path in list_files]\n",
        "    invalid_images = []\n",
        "    for i, im in enumerate(tqdm.tqdm(filenames)):\n",
        "      # check if the 1st, 2nd, 3rd parts of a given file name are digits\n",
        "      # As a recap, image name goes like 1_1_1_00000, \n",
        "# which means age_gender_race_unrelated info\n",
        "        if im[0].isdigit() and im[1].isdigit() and im[2].isdigit():\n",
        "            continue\n",
        "        else:\n",
        "            invalid_images.append(list_files[i])\n",
        "    return invalid_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvPN2c8CsrHW"
      },
      "source": [
        "# check # of invalid images\n",
        "invalid_images = get_invalid_images()\n",
        "print(invalid_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL_9a5erIPED"
      },
      "source": [
        "### remove unwanted races & manipulate the # of images for each race"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIjfs2dkuDa9"
      },
      "source": [
        "# function to remove unwanted races (in our case, only include black & white faces)\n",
        "# change path based on your needs\n",
        "def get_bw_images(root_path='/content/data/ff/'):\n",
        "    list_files = glob.glob(os.path.join(root_path, '*.[jJ][pP]*'))\n",
        "    filenames = [path.split('/')[-1].split('_') for path in list_files]\n",
        "    invalid_races = []\n",
        "    #find images based on its names\n",
        "    for i, im in enumerate(tqdm.tqdm(filenames)):\n",
        "      #only include 0&1, which means black & white\n",
        "      # As a recap, image name goes like 1_1_1_00000, \n",
        "# which means age_gender_race_unrelated info\n",
        "        if im[2] == '0' or im[2] == '1':\n",
        "            continue\n",
        "        else:\n",
        "            invalid_races.append(list_files[i])\n",
        "\n",
        "    return invalid_races"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ7c-_iKuuNw"
      },
      "source": [
        "# check # of faces from unwanted races\n",
        "invalid_races = get_bw_images()\n",
        "print(len(invalid_races))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29YQ2IGMsvlD"
      },
      "source": [
        "# Remove invalid files\n",
        "for f in invalid_races:\n",
        "    os.remove(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMv1bevxy0S-"
      },
      "source": [
        "# change # of faces for black & white faces based on our needs\n",
        "black_list = []\n",
        "white_list = []\n",
        "# change based on your needs\n",
        "def get_ratio(root_path='/content/data/ff/'):\n",
        "    list_files = glob.glob('/content/data/ff/*jp*')\n",
        "#find images based on its names\n",
        "    filenames = [path.split('/')[-1].split('_') for path in list_files]\n",
        "    invalid_white = []\n",
        "    invalid_black = []\n",
        "    for t, tm in enumerate(tqdm.tqdm(filenames)):\n",
        "#check if the image is black\n",
        "# As a recap, image name goes like 1_1_1_00000, \n",
        "# which means age_gender_race_unrelated info\n",
        "      if tm[2] =='1':\n",
        "        black_list.append(list_files[t])\n",
        "        #stop adding black faces when reaching 10000. Change the number based on your need\n",
        "        if len(black_list)>10000:\n",
        "          # add unwanted blacks to one list\n",
        "          invalid_black = black_list[10000:] \n",
        "  # same goes for the white faces\n",
        "      elif tm[2] =='0':\n",
        "        white_list.append(list_files[t])\n",
        "        if len(white_list)>5000:\n",
        "          invalid_white = white_list[5000:]\n",
        "\n",
        "# record unwanted faces\n",
        "    return invalid_black, invalid_white \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM8txsPVzCdB"
      },
      "source": [
        "invalid_black, invalid_white = get_ratio()\n",
        "# check the # of unwanted faces for each race\n",
        "print(len(invalid_white))\n",
        "print(len(invalid_black))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRX9xsI-zFl9"
      },
      "source": [
        "# remove unwanted faces from the set\n",
        "for f in invalid_white:\n",
        "    os.remove(f)\n",
        "\n",
        "for f in invalid_black:\n",
        "    os.remove(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wddsk9uzKJdO"
      },
      "source": [
        "### double check "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1kG9U8QKnWF",
        "outputId": "8cdf200f-7d79-445f-9d2f-4893e16507d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# double check # of faces for each races \n",
        "\n",
        "def display_examples_utk(root_path, label_type, label_value):\n",
        "# add labels based on the digit of the file name. As a recap, image name goes like 1_1_1_00000, \n",
        "# which means age_gender_race_unrelated info\n",
        "    list_files = [item for item in glob.glob(os.path.join(root_path, '*.jpg'))\n",
        "                  if len(item.split('/')[-1].split('_')[:-1]) == 3]\n",
        "\n",
        "    labels = [item.split('/')[-1].split('_')[:-1] for item in list_files]\n",
        "# create labels\n",
        "    labels = {\n",
        "        'age': [int(item[0]) for item in labels],\n",
        "        'gender': [int(item[1]) for item in labels],\n",
        "        'race': [int(item[2]) for item in labels]\n",
        "    }\n",
        "# name the labels\n",
        "    label_names = {\n",
        "        \"race\": {0: 'White', 1: 'Black', 2: 'East Asian', 3: 'Southeast Asian', 4: 'Middle Eastern', 5: 'Latino_Hispanic', 6:'Indian'},\n",
        "        \"gender\": {0: 'Male', 1: 'Female'},\n",
        "    }\n",
        "\n",
        "# list the # of faces for each race\n",
        "    print('Number of images for {} : {}'.format(\n",
        "        label_type, label_names[label_type][label_value] if label_type != 'age' else label_value\n",
        "    ))\n",
        "    inds = [ind for ind, lab in enumerate(labels[label_type]) if lab == label_value]\n",
        "\n",
        "    print(len(inds))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  display_examples_utk('/content/data/ff/', 'race', 0)\n",
        "  display_examples_utk('/content/data/ff/', 'race', 1)\n",
        "  display_examples_utk('/content/data/ff/', 'race', 2)\n",
        "  display_examples_utk('/content/data/ff/', 'race', 3)\n",
        "  display_examples_utk('/content/data/ff/', 'race', 4)\n",
        "  display_examples_utk('/content/data/ff/', 'race', 5)\n",
        "  display_examples_utk('/content/data/ff/', 'race', 6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images for race : White\n",
            "5000\n",
            "Number of images for race : Black\n",
            "10000\n",
            "Number of images for race : East Asian\n",
            "0\n",
            "Number of images for race : Southeast Asian\n",
            "0\n",
            "Number of images for race : Middle Eastern\n",
            "0\n",
            "Number of images for race : Latino_Hispanic\n",
            "0\n",
            "Number of images for race : Indian\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Q6N6Ga6-Gh"
      },
      "source": [
        "## 2. Some data pre-processing operations\n",
        "\n",
        "Now that we have our dataset ready let's split it into training (70%), test (15%) and validation (15%) sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ_WKloZt1vz"
      },
      "source": [
        "# split the dataset into train, test and validation sets \n",
        "SRC_DIR = '/content/data/ff/'  # path to the folder containing all images\n",
        "DEST_DIR = '/content/data/ff_split/' # path where to save the split dataset, 3 subdirectories will be created (train, valid and test)\n",
        "SPLIT = 0.7 # ratio of the train set, the remaining (30%) will be split equally between validation and test sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfGSvdSXuTkv",
        "outputId": "07d1cb54-9add-42d7-d895-d0f2876fc720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "split_utk(SRC_DIR, DEST_DIR, SPLIT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 921/10499 [00:00<00:01, 9205.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------Copying train images-------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10499/10499 [00:01<00:00, 9893.12it/s]\n",
            " 19%|█▉        | 433/2250 [00:00<00:00, 4319.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------Copying valid images-------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [00:00<00:00, 5495.77it/s]\n",
            " 35%|███▌      | 794/2251 [00:00<00:00, 7648.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------Copying test images-------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2251/2251 [00:00<00:00, 4808.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEYIhW97JQHN"
      },
      "source": [
        "Let's also define some transformations we would like to apply :\n",
        "* Resize all images to 128 x 128\n",
        "* convert them to pytorch tensors before feeding to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07NvuKaBvJie"
      },
      "source": [
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFoPIGJKfC-"
      },
      "source": [
        "The last step before training is to create  DataLoader objects, which are pytorch generator-like objects for yielding data into batches during training :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K30DAfY2tmIU"
      },
      "source": [
        "train_loader = get_utk_dataloader(batch_size=128, data_dir=DEST_DIR, data_transforms=data_transforms, flag='train')\n",
        "val_loader = get_utk_dataloader(batch_size=128, data_dir=DEST_DIR, data_transforms=data_transforms, flag='valid')\n",
        "\n",
        "my_data_loaders = {\n",
        "    'train': train_loader,\n",
        "    'valid': val_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2E7a5w7LD0l"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFldsxJeLcDb"
      },
      "source": [
        "We'll trry three different architectures : \n",
        "* CNN based on Depthwise separable convolutions blocks\n",
        "* Finetuning a pretrained Resnet50\n",
        "* Finetuning a pretrained VGG19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQeEdKHtLQpY"
      },
      "source": [
        "### 3.1 Deptwhise Separable Convolutions architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjuzVpeYwPeA"
      },
      "source": [
        "# Sepconv with adam\n",
        "\n",
        "my_model = SepConvModelMT(dropout=0.7, n_class=[1, 2, 7], n_filters=[64, 128, 256, 512], kernels_size=[3, 3, 3, 3])\n",
        "my_optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_mbLvWRL7ao"
      },
      "source": [
        "We have created above a model and an Adam optimizer. The model has the following parameters :\n",
        "* `dropout` = 0.7 : we apply a droput of 70%\n",
        "* `n_class` = [1, 2, 6] : the model has 3 outputs that are age (a scalar thus a shape of 1), gender (male or female thus a shape of 2) and race (white, black, asian, indian or other thus a shape of 6).\n",
        "*`n_filters` =  [64, 128, 256, 512] : number of features maps for each conv block\n",
        "* `kernels_size` = [3, 3, 3, 3] : conv kernel size for each conv block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25jpOuvnNieu"
      },
      "source": [
        "I also need to periodically backup the model's checkpoints in my google drive in case I get disconnected to google colab due to connectivity issues or anything else :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q1zRw85xcVF"
      },
      "source": [
        "backup_path = \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/sep_conv_adam\"\n",
        "os.makedirs(backup_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnRkLNkvOa7N"
      },
      "source": [
        "Before starting training, let's clarify some important input arguments to the training function that are not quite clear :\n",
        "* `log_interval` : print the training loss each `log_interval` iterations\n",
        "* `dirname` : path to the directory where to locally save the best model checkpoint after each epoch\n",
        "* `filename_prefix` : file name under which to save the model checkpoint\n",
        "* `n_saved` : save the `n_saved` best models\n",
        "* `launch_tensorboard` and `log_dir` :  whether to write tensorboard summaries, and if True, write them under `log_dir` folder\n",
        "* `patience`: number of epochs to wait for before stopping training if no improvement is observed\n",
        "* `resume_model` and `resume_optimizer` : optional paths to previously trained model and optimizer to start use them as starting point for the training\n",
        "* `backup_step` and `backup_path` : copy the saved checkpoints from `dirname` to `backup_path` each `backup_step` epochs\n",
        "* `n_epochs_freeze` : unfreeze the frozen layers after `n_epochs_freeze`, this is particularly used in case of finetuning a pretrained model\n",
        "* `lr_after_freeze` : in case of finetuning a pretrained model, new learning rate to set after unfreezing frozen layers\n",
        "* `loss_weights` : the model's outputs (age, gender, race) are not of the same magnitude, wo we made need to assign them different weights representing their respective contributions to the global loss\n",
        "\n",
        "\n",
        "\n",
        "Now that it's a little bit clearer let's start training :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhUlXkCcw3VT"
      },
      "source": [
        "run_utk(my_model, my_optimizer, epochs=300, log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname='/content/checkpoints/sep_conv_adam', filename_prefix='sep_conv_adam', n_saved=1,\n",
        "        log_dir=None, launch_tensorboard=False, patience=50,\n",
        "        resume_model=None, resume_optimizer=None, backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=0, lr_after_freeze=None,\n",
        "        loss_weights=[1/10, 1/0.16, 1/0.44])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0fd6xS6S1qw"
      },
      "source": [
        "### 3.2 Finetunig a pretrained Resnet50\n",
        "\n",
        "We are going to :\n",
        "* freeze all other layers and only train the classification using a learning rate of 1e-3\n",
        "* unfreeze all layers after 10 epochs and finetune the whole model using a learning rate of 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JylvWdOnJmG"
      },
      "source": [
        "backup_path = \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/9010/resnet_adam\"\n",
        "os.makedirs(backup_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d31f4MrQmGxa"
      },
      "source": [
        "num_classes=3\n",
        "my_model = PretrainedMT(model_name='resnet', feature_extract=True, use_pretrained=True)\n",
        "my_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, my_model.parameters()), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9RZTjpZCvxk"
      },
      "source": [
        "import sys\n",
        "sys.path.append('../../vision_utils')\n",
        "import torch.optim as optim\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "my_model = ConvModelMultiTask()\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(\n",
        "    [\n",
        "        {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "    ],\n",
        "    lr=1e-6,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEPIr3CgApjM"
      },
      "source": [
        "class PretrainedMT(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPMvktmyncQ1"
      },
      "source": [
        "run_utk(my_model, my_optimizer, epochs=300, log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname='/content/checkpoints/resnet_adam', filename_prefix='resnet', n_saved=1,\n",
        "        log_dir='/content/logs', launch_tensorboard=False, patience=50,\n",
        "        resume_model=None, resume_optimizer=None, backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=10, n_cycle=None, lr_after_freeze=1e-4,\n",
        "        loss_weights=[1/10, 1/0.16, 1/0.44], lr_plot=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q45G7NAbUeu9"
      },
      "source": [
        "### 3.3 Finetuning a pretrained VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1c4Z5xpePkl"
      },
      "source": [
        "backup_path = \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/vgg_adam\"\n",
        "os.makedirs(backup_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8cAnX-eePkp"
      },
      "source": [
        "my_model = PretrainedMT(model_name='vgg', feature_extract=True, use_pretrained=True)\n",
        "my_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, my_model.parameters()), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGuIp90fePkv"
      },
      "source": [
        "run_utk(my_model, my_optimizer, epochs=300, log_interval=1, dataloaders=my_data_loaders,\n",
        "        dirname='/content/checkpoints/vgg_adam', filename_prefix='vgg', n_saved=1,\n",
        "        log_dir='/content/logs', launch_tensorboard=False, patience=50,\n",
        "        resume_model=None, resume_optimizer=None, backup_step=5, backup_path=backup_path,\n",
        "        n_epochs_freeze=10, n_cycle=None, lr_after_freeze=1e-4,\n",
        "        loss_weights=[1/10, 1/0.16, 1/0.44], lr_plot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YbhF27yVNWF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4afPqkJNVR2p"
      },
      "source": [
        "## 4. Evaluation\n",
        "\n",
        "Now that our 3 models are trained we can evaluate them on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKSDOR02Z2_j"
      },
      "source": [
        "# check the checkpoints stored in the directory\n",
        "!ls \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/cam/vgg_adam/white\"\n",
        "# !ls \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/sep_conv_adam\"\n",
        "# !ls \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/vgg_adam\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWc-PZXUVe2R"
      },
      "source": [
        "# create test data loader\n",
        "test_loader = get_utk_dataloader(batch_size=256, data_dir=DEST_DIR, data_transforms=data_transforms, flag='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da6oP4rhZyv0"
      },
      "source": [
        "# Load the three models\n",
        "\n",
        "# paths to the saved models\n",
        "# path_sep_conv = \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/sep_conv_adam/sep_conv_adam_checkpoint_val_loss=-4.26717553605379.pth\"\n",
        "path_resnet = \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/9010/resnet_adam/resnet_checkpoint_val_loss=-4.02144347319082.pth\"\n",
        "# path_vgg = \"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/vgg_adam/vgg_checkpoint_val_loss=-4.070847482863642.pth\"\n",
        "\n",
        "cpu_or_gpu = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# sep_conv_model = SepConvModelMT()\n",
        "# sep_conv_model.load_state_dict(torch.load(path_sep_conv, map_location=cpu_or_gpu)['model'])\n",
        "\n",
        "import sys\n",
        "sys.path.append('../../vision_utils')\n",
        "import torch.optim as optim\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "my_model = ConvModelMultiTask()\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(\n",
        "    [\n",
        "        {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "        {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "        {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "    ],\n",
        "    lr=1e-6,\n",
        ")\n",
        "\n",
        "class PretrainedMT(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "resnet_model = PretrainedMT(model_name='resnet')\n",
        "resnet_model.load_state_dict(torch.load(path_resnet, map_location=cpu_or_gpu)['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWW3qOmwdy3T"
      },
      "source": [
        "### Evaluate model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ymWXjKqhAl0"
      },
      "source": [
        "# Import\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from vision_utils.custom_torch_utils import plot_confusion_matrix\n",
        "from vision_utils.custom_torch_utils import processing_time\n",
        "from torchvision import transforms\n",
        "import time\n",
        "\n",
        "def processing_time(func):\n",
        "    \"\"\"\n",
        "    utility function to print execution time of a given function\n",
        "\n",
        "    :param func: a python function to track the execution time\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    def func_wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        seconds = time.time() - start\n",
        "        m, s = divmod(seconds, 60)\n",
        "        h, m = divmod(m, 60)\n",
        "        print(f\"The execution took {h} hours | {m} minutes | {s:.1f} seconds!\")\n",
        "    return func_wrapper\n",
        "\n",
        "\n",
        "@processing_time\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6UxSpvNNyOp"
      },
      "source": [
        "### Evaluate separable convolution model : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUYupEgNZp14"
      },
      "source": [
        "evaluate_model(sep_conv_model, test_loader, title='Evaluation on test set - Separable conv model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKGkVc0wiZzR"
      },
      "source": [
        "### Evaluate resnet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjwnrvI4icX9"
      },
      "source": [
        "evaluate_model(resnet_model, test_loader, title='Evaluation on test set - Resnet conv model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEBDdKt6i_ed"
      },
      "source": [
        "### Evaluate VGG model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km-8_WlBjEsY"
      },
      "source": [
        "evaluate_model(vgg_model, test_loader, title='Evaluation on test set - VGG model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKrnQAzP745Z"
      },
      "source": [
        "### normal-male"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6je13kK37Uns"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.249345162580937.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "                race_text = f\"{race_lab, round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for v in range(11):\n",
        "      image_list = []\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mor_male/m'+ str(v+1)\n",
        "      # image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/M/morphed_'+ str(v+1)\n",
        "      # image_id = '/Users/a3055/Desktop/mor_male/m' + str(v + 1)\n",
        "\n",
        "      for i in range(11):\n",
        "          if i < 10:\n",
        "              images = image_id+ '_0'+ str(i) + '.jpg'\n",
        "          else:\n",
        "              images = image_id+ '_10.jpg'\n",
        "\n",
        "          \n",
        "          source_file = images\n",
        "\n",
        "\n",
        "          frame = cv2.imread(source_file)\n",
        "          frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "          parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "          cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_m_res/', f_name+'_predicted.jpg'), frame)\n",
        "          cv2_imshow(frame)\n",
        "          key = cv2.waitKey(0) & 0x00\n",
        "          if key == ord(\"q\"):\n",
        "            cv2.destroyAllWindows()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIN37LPApf_o"
      },
      "source": [
        "###normal-female\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgj9oTMypmLo"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.249345162580937.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            # do age, gender and race prediction\n",
        "            # face = frame[startY-25: endY+25, startX-25: endX+25]\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "                race_text = f\"{race_lab, round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "    for v in range(11):\n",
        "      image_list = []\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mor_female/F'+ str(v+1)\n",
        "\n",
        "\n",
        "      for i in range(11):\n",
        "          if i < 10:\n",
        "              images = image_id+ '_0'+ str(i) + '.jpg'\n",
        "          else:\n",
        "              images = image_id+ '_10.jpg'\n",
        "\n",
        "          image_list.append(images)         \n",
        "          source_file = images\n",
        "\n",
        "\n",
        "\n",
        "          frame = cv2.imread(source_file)\n",
        "          frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "          parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "          cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f_res/', f_name+'_predicted.jpg'), frame)\n",
        "          cv2_imshow(frame)\n",
        "          # cv2_imshow('Face Detector', frame)\n",
        "          key = cv2.waitKey(0) & 0x00\n",
        "          if key == ord(\"q\"):\n",
        "            cv2.destroyAllWindows()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvikiTnNMKgO"
      },
      "source": [
        "###SINGLE IMAGE TEST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1b5rkRhPq57"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/9010/resnet_adam/resnet_checkpoint_val_loss=-4.02144347319082.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            # do age, gender and race prediction\n",
        "            # face = frame[startY-25: endY+25, startX-25: endX+25]\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "                race_text = f\"{race_lab, round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.3, (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "    source_file = \"/content/gdrive/My Drive/DeepLearning/Face_detection/test/test25.jpg\"\n",
        "    frame = cv2.imread(source_file)\n",
        "\n",
        "    frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "    parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "    cv2.imwrite(os.path.join(parent, f_name+'_predicted.jpg'), frame)\n",
        "    cv2_imshow(frame)\n",
        "    # cv2_imshow('Face Detector', frame)\n",
        "    key = cv2.waitKey(0) & 0x00\n",
        "    if key == ord(\"q\"):\n",
        "      cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNxphiFawFCQ"
      },
      "source": [
        "### PSE-Male\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7gsLf3gv-82"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "import pandas as pd\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/9010/resnet_adam/resnet_checkpoint_val_loss=-4.02144347319082.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            # do age, gender and race prediction\n",
        "            # face = frame[startY-25: endY+25, startX-25: endX+25]\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "                race_text = f\"{race_lab, round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame, race_text\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    image_list = []\n",
        "    res_list=[]\n",
        "    fname_list=[]\n",
        "    key_list = []\n",
        "    res_list1=[]\n",
        "    fname_list1=[]\n",
        "    key_list1 = []\n",
        "\n",
        "    for v in range(10):\n",
        "\n",
        "      # image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_m/M'+ str(v)\n",
        "\n",
        "      for i in range(11):\n",
        "        images = image_id+ '_'+ str(i) + '.jpg'\n",
        "\n",
        "        source_file = images\n",
        "\n",
        "\n",
        "\n",
        "        frame = cv2.imread(source_file)\n",
        "        frame,race_text = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "        parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "  \n",
        "\n",
        "        cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010M/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "\n",
        "        cv2_imshow(frame)\n",
        "        # cv2_imshow('Face Detector', frame)\n",
        "        key = cv2.waitKey(0) & 0x00\n",
        "        if key == ord(\"q\"):\n",
        "          cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "        if race_text[2:7] == 'White':\n",
        "          race_digit= 0\n",
        "        else:\n",
        "          race_digit= 1\n",
        "        res_list.append(race_digit)\n",
        "\n",
        "        fname_list.append(f_name)\n",
        "\n",
        "        temp = f_name.strip('.jpg')\n",
        "        temp = int(temp[3:])*10\n",
        "        print(str(temp))\n",
        "        key_list.append(temp)\n",
        "        \n",
        "\n",
        "        df = pd.DataFrame(\n",
        "                {'id': ['9010M']*len(fname_list),\n",
        "                'stim.name': fname_list,\n",
        "                'choice': res_list})\n",
        "        \n",
        "        df.append(df)\n",
        "\n",
        "        df_key = pd.DataFrame(\n",
        "                {'stim.name': fname_list,\n",
        "                'Racial.Percentage': key_list}) \n",
        "\n",
        "        df_key.append(df_key)\n",
        "        print(df_key)\n",
        "\n",
        "        df.to_csv('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010M/m_df.csv',  encoding='utf-8')\n",
        "        df_key.to_csv('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010M/m_key.csv',  encoding='utf-8')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZTmITAyRTIB"
      },
      "source": [
        "### mask - male\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DChQiVQGMUq2"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/9010/resnet_adam/resnet_checkpoint_val_loss=-4.02144347319082.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            # do age, gender and race prediction\n",
        "            # face = frame[startY-25: endY+25, startX-25: endX+25]\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "                race_text = f\"{race_lab, round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for v in range(11):\n",
        "      image_list = []\n",
        "\n",
        "      # image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_m/M'+ str(v)\n",
        "\n",
        "      for i in range(11):\n",
        "        images = image_id+ '_'+ str(i) + '.jpg'\n",
        "\n",
        "        source_file = images\n",
        "\n",
        "\n",
        "\n",
        "        frame = cv2.imread(source_file)\n",
        "        frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "\n",
        "        parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "        cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010M/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "        cv2_imshow(frame)\n",
        "        # cv2_imshow('Face Detector', frame)\n",
        "        key = cv2.waitKey(0) & 0x00\n",
        "        if key == ord(\"q\"):\n",
        "          cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKH3xz6psDPU"
      },
      "source": [
        "### mask-female\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsUvf8_msJQr"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/9010/resnet_adam/resnet_checkpoint_val_loss=-4.02144347319082.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black']],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black']\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            # do age, gender and race prediction\n",
        "            # face = frame[startY-25: endY+25, startX-25: endX+25]\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "                race_text = f\"{race_lab, round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame, race_text\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    image_list = []\n",
        "    res_list=[]\n",
        "    fname_list=[]\n",
        "    key_list = []\n",
        "    res_list1=[]\n",
        "    fname_list1=[]\n",
        "    key_list1 = []\n",
        "\n",
        "    for v in range(10):\n",
        "      image_list = []\n",
        "\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "      # image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_m/M'+ str(v)\n",
        "\n",
        "      for i in range(11):\n",
        "        images = image_id+ '_'+ str(i) + '.jpg'\n",
        "\n",
        "        source_file = images\n",
        "\n",
        "        frame = cv2.imread(source_file)\n",
        "        frame,race_text = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "\n",
        "        parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "        cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010F/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "        cv2_imshow(frame)\n",
        "        # cv2_imshow('Face Detector', frame)\n",
        "        key = cv2.waitKey(0) & 0x00\n",
        "        if key == ord(\"q\"):\n",
        "          cv2.destroyAllWindows()\n",
        "        \n",
        "\n",
        "        ######################################################################\n",
        "        if race_text[2:7] == 'White':\n",
        "          race_digit= 0\n",
        "        else:\n",
        "          race_digit= 1\n",
        "        res_list.append(race_digit)\n",
        "\n",
        "        fname_list.append(f_name)\n",
        "\n",
        "        temp = f_name.strip('.jpg')\n",
        "        temp = int(temp[3:])*10\n",
        "        print(str(temp))\n",
        "        key_list.append(temp)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "               {'id': ['9010M']*len(fname_list),\n",
        "                'stim.name': fname_list,\n",
        "                'choice': res_list})\n",
        "        \n",
        "\n",
        "\n",
        "        df_key = pd.DataFrame(\n",
        "               {'stim.name': fname_list,\n",
        "                'Racial.Percentage': key_list})    \n",
        "        print(df_key)\n",
        "\n",
        "        df.to_csv('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010F/f_df.csv',  encoding='utf-8')\n",
        "        df_key.to_csv('/content/gdrive/My Drive/DeepLearning/Face_detection/test/9010F/f_key.csv',  encoding='utf-8')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_OSCx19xur"
      },
      "source": [
        "### B/W VERSION - male\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi465Mmz92YK"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.249345162580937.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 3)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "######b/w version             \n",
        "                if race_proba[0] > race_proba[1]:\n",
        "                  white_prob = round(race_proba[0],3)\n",
        "                  race_text = f\"{'White', white_prob}\"\n",
        "                else: \n",
        "                  race_lab = race_labels[1]                  \n",
        "                  black_prob = round(race_proba[1],3)\n",
        "                  race_text = f\"{'Black', black_prob}\"\n",
        "######b/w version\n",
        "                # race_text = f\"{'test', round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "    for v in range(11):\n",
        "      image_list = []\n",
        "\n",
        "      # image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_m/M'+ str(v)\n",
        "\n",
        "\n",
        "      for i in range(11):\n",
        "        images = image_id+ '_'+ str(i) + '.jpg'\n",
        "        \n",
        "          \n",
        "        source_file = images\n",
        "\n",
        "\n",
        "        frame = cv2.imread(source_file)\n",
        "        frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "\n",
        "        parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "        cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_m_bw_res/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "        cv2_imshow(frame)\n",
        "        key = cv2.waitKey(0) & 0x00\n",
        "        if key == ord(\"q\"):\n",
        "          cv2.destroyAllWindows()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuVD6A-MfMhW"
      },
      "source": [
        "### B/W Version - female"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyy66JwyfPqb"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.249345162580937.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 1)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "######b/w version             \n",
        "                if race_proba[0] > race_proba[1]:\n",
        "                  white_prob = round(race_proba[0],3)\n",
        "                  race_text = f\"{'White', white_prob}\"\n",
        "                else: \n",
        "                  race_lab = race_labels[1]                  \n",
        "                  black_prob = round(race_proba[1],3)\n",
        "                  race_text = f\"{'Black', black_prob}\"\n",
        "######b/w version\n",
        "                # race_text = f\"{'test', round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for v in range(10):\n",
        "      image_list = []\n",
        "\n",
        "      image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "\n",
        "\n",
        "      for i in range(11):\n",
        "        images = image_id+ '_'+ str(i) + '.jpg'\n",
        "        \n",
        "          \n",
        "        source_file = images\n",
        "\n",
        "        frame = cv2.imread(source_file)\n",
        "        frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "\n",
        "        parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "\n",
        "        cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f_bw_res/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "        cv2_imshow(frame)\n",
        "        # cv2_imshow('Face Detector', frame)\n",
        "        key = cv2.waitKey(0) & 0x00\n",
        "        if key == ord(\"q\"):\n",
        "          cv2.destroyAllWindows()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnSuRR4vqLK"
      },
      "source": [
        "## GRAD-CAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqqRhIHnNOlJ"
      },
      "source": [
        "### warning: this is a mess. Ignore this section for now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZJo2fwwtud"
      },
      "source": [
        "\n",
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "from torch.autograd import Function\n",
        "from torchvision import models\n",
        "import json\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.249345162580937.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 1)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "######b/w version             \n",
        "                if race_proba[0] > race_proba[1]:\n",
        "                  white_prob = round(race_proba[0],3)\n",
        "                  race_text = f\"{'White', white_prob}\"\n",
        "                else: \n",
        "                  race_lab = race_labels[1]                  \n",
        "                  black_prob = round(race_proba[1],3)\n",
        "                  race_text = f\"{'Black', black_prob}\"\n",
        "######b/w version\n",
        "                # race_text = f\"{'test', round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "##########grad-cam starts\n",
        "\n",
        "# class FeatureExtractor():\n",
        "#     \"\"\" Class for extracting activations and \n",
        "#     registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "#     def __init__(self, model, target_layers):\n",
        "#         self.model = model\n",
        "#         self.target_layers = target_layers\n",
        "#         self.gradients = []\n",
        "\n",
        "#     def save_gradient(self, grad):\n",
        "#         self.gradients.append(grad)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         outputs = []\n",
        "#         self.gradients = []\n",
        "#         for name, module in self.model._modules.items():\n",
        "#             x = module(x)\n",
        "#             if name in self.target_layers:\n",
        "#                 x.register_hook(self.save_gradient)\n",
        "#                 outputs += [x]\n",
        "#         return outputs, x\n",
        "\n",
        "\n",
        "# class ModelOutputs():\n",
        "#     \"\"\" Class for making a forward pass, and getting:\n",
        "#     1. The network output.\n",
        "#     2. Activations from intermeddiate targetted layers.\n",
        "#     3. Gradients from intermeddiate targetted layers. \"\"\"\n",
        "\n",
        "#     def __init__(self, model, feature_module, target_layers):\n",
        "#         self.model = model\n",
        "#         self.feature_module = feature_module\n",
        "#         self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)\n",
        "\n",
        "#     def get_gradients(self):\n",
        "#         return self.feature_extractor.gradients\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         target_activations = []\n",
        "#         for name, module in self.model._modules.items():\n",
        "#             if module == self.feature_module:\n",
        "#                 target_activations, x = self.feature_extractor(x)\n",
        "#             elif \"avgpool\" in name.lower():\n",
        "#                 x = module(x)\n",
        "#                 x = x.view(x.size(0),-1)\n",
        "#             else:\n",
        "#                 x = module(x)\n",
        "        \n",
        "#         return target_activations, x\n",
        "\n",
        "\n",
        "# def preprocess_image(img):\n",
        "#     means = [0.485, 0.456, 0.406]\n",
        "#     stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "#     preprocessed_img = img.copy()[:, :, ::-1]\n",
        "#     for i in range(3):\n",
        "#         preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "#         preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "#     preprocessed_img = \\\n",
        "#         np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "#     preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "#     preprocessed_img.unsqueeze_(0)\n",
        "#     input = preprocessed_img.requires_grad_(True)\n",
        "#     return input\n",
        "\n",
        "\n",
        "# def show_cam_on_image(img, mask):\n",
        "#     heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "#     heatmap = np.float32(heatmap) / 255\n",
        "#     cam = heatmap + np.float32(img)\n",
        "#     cam = cam / np.max(cam)\n",
        "#     cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
        "\n",
        "\n",
        "# class GradCam:\n",
        "#     def __init__(self, model, feature_module, target_layer_names, use_cuda):\n",
        "#         self.model = model\n",
        "#         self.feature_module = feature_module\n",
        "#         self.model.eval()\n",
        "#         self.cuda = use_cuda\n",
        "#         if self.cuda:\n",
        "#             self.model = model.cuda()\n",
        "\n",
        "#         self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self.model(input)\n",
        "\n",
        "#     def __call__(self, input, index=None):\n",
        "#         if self.cuda:\n",
        "#             features, output = self.extractor(input.cuda())\n",
        "#         else:\n",
        "#             features, output = self.extractor(input)\n",
        "\n",
        "#         if index == None:\n",
        "#             index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "#         one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "#         one_hot[0][index] = 1\n",
        "#         one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "#         if self.cuda:\n",
        "#             one_hot = torch.sum(one_hot.cuda() * output)\n",
        "#         else:\n",
        "#             one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "#         self.feature_module.zero_grad()\n",
        "#         self.model.zero_grad()\n",
        "#         one_hot.backward(retain_graph=True)\n",
        "\n",
        "#         grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
        "\n",
        "#         target = features[-1]\n",
        "#         target = target.cpu().data.numpy()[0, :]\n",
        "\n",
        "#         weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
        "#         cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
        "\n",
        "#         for i, w in enumerate(weights):\n",
        "#             cam += w * target[i, :, :]\n",
        "\n",
        "#         cam = np.maximum(cam, 0)\n",
        "#         cam = cv2.resize(cam, input.shape[2:])\n",
        "#         cam = cam - np.min(cam)\n",
        "#         cam = cam / np.max(cam)\n",
        "#         return cam\n",
        "\n",
        "\n",
        "# class GuidedBackpropReLU(Function):\n",
        "\n",
        "#     @staticmethod\n",
        "#     def forward(self, input):\n",
        "#         positive_mask = (input > 0).type_as(input)\n",
        "#         output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
        "#         self.save_for_backward(input, output)\n",
        "#         return output\n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(self, grad_output):\n",
        "#         input, output = self.saved_tensors\n",
        "#         grad_input = None\n",
        "\n",
        "#         positive_mask_1 = (input > 0).type_as(grad_output)\n",
        "#         positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
        "#         grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),\n",
        "#                                    torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,\n",
        "#                                                  positive_mask_1), positive_mask_2)\n",
        "\n",
        "#         return grad_input\n",
        "\n",
        "\n",
        "# class GuidedBackpropReLUModel:\n",
        "#     def __init__(self, model, use_cuda):\n",
        "#         self.model = model\n",
        "#         self.model.eval()\n",
        "#         self.cuda = use_cuda\n",
        "#         if self.cuda:\n",
        "#             self.model = model.cuda()\n",
        "\n",
        "#         def recursive_relu_apply(module_top):\n",
        "#             for idx, module in module_top._modules.items():\n",
        "#                 recursive_relu_apply(module)\n",
        "#                 if module.__class__.__name__ == 'ReLU':\n",
        "#                     module_top._modules[idx] = GuidedBackpropReLU.apply\n",
        "                \n",
        "#         # replace ReLU with GuidedBackpropReLU\n",
        "#         recursive_relu_apply(self.model)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self.model(input)\n",
        "\n",
        "#     def __call__(self, input, index=None):\n",
        "#         if self.cuda:\n",
        "#             output = self.forward(input.cuda())\n",
        "#         else:\n",
        "#             output = self.forward(input)\n",
        "\n",
        "#         if index == None:\n",
        "#             index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "#         one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "#         one_hot[0][index] = 1\n",
        "#         one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "#         if self.cuda:\n",
        "#             one_hot = torch.sum(one_hot.cuda() * output)\n",
        "#         else:\n",
        "#             one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "#         # self.model.features.zero_grad()\n",
        "#         # self.model.classifier.zero_grad()\n",
        "#         one_hot.backward(retain_graph=True)\n",
        "\n",
        "#         output = input.grad.cpu().data.numpy()\n",
        "#         output = output[0, :, :, :]\n",
        "\n",
        "#         return output\n",
        "\n",
        "\n",
        "# def get_args():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--use-cuda', action='store_true', default=True,\n",
        "#                         help='Use NVIDIA GPU acceleration')\n",
        "#     parser.add_argument('--image-path', type=str, default='./examples/both.png',\n",
        "#                         help='Input image path')\n",
        "#     args = parser.parse_args()\n",
        "#     args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
        "#     if args.use_cuda:\n",
        "#         print(\"Using GPU for acceleration\")\n",
        "#     else:\n",
        "#         print(\"Using CPU for computation\")\n",
        "\n",
        "#     return args\n",
        "\n",
        "# def deprocess_image(img):\n",
        "#     \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
        "#     img = img - np.mean(img)\n",
        "#     img = img / (np.std(img) + 1e-5)\n",
        "#     img = img * 0.1\n",
        "#     img = img + 0.5\n",
        "#     img = np.clip(img, 0, 1)\n",
        "#     return np.uint8(img*255)\n",
        "###########grad-cam ends\n",
        "\n",
        "\n",
        "\n",
        "###########grad-cam starts\n",
        "\n",
        "\n",
        "class FeatureExtractor():\n",
        "    \"\"\" Class for extracting activations and \n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model, pre_features, features, target_layers):\n",
        "        self.model = model\n",
        "        self.pre_features = pre_features\n",
        "        self.features = features\n",
        "        self.target_layers = target_layers\n",
        "        self.gradients = []\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients.append(grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outputs = []\n",
        "        self.gradients = []\n",
        "        \n",
        "        for pref in self.pre_features:\n",
        "            x = getattr(self.model, pref)(x)\n",
        "\n",
        "        submodel = getattr(self.model, self.features)\n",
        "        # go through the feature extractor's forward pass\n",
        "        for name, module in submodel._modules.items():\n",
        "            # print(name, module)\n",
        "            x = module(x)\n",
        "            if name in self.target_layers:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                outputs += [x]\n",
        "        return outputs, x\n",
        "\n",
        "\n",
        "class ModelOutputs():\n",
        "    \"\"\" Class for making a forward pass, and getting:\n",
        "    1. The network output.\n",
        "    2. Activations from intermeddiate targetted layers.\n",
        "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
        "\n",
        "    def __init__(self, model,\n",
        "                 pre_feature_block=[],\n",
        "                 feature_block='features',\n",
        "                 target_layers='35',\n",
        "                 classifier_block=['classifier']):\n",
        "        self.model = model\n",
        "        self.classifier_block = classifier_block\n",
        "        # assume the model has a module named `feature`      ⬇⬇⬇⬇⬇⬇⬇⬇\n",
        "        self.feature_extractor = FeatureExtractor(self.model,\n",
        "                                                  pre_feature_block,\n",
        "                                                  feature_block,\n",
        "                                                  target_layers)\n",
        "\n",
        "    def get_gradients(self):\n",
        "        return self.feature_extractor.gradients\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # ⬇ target layer    ⬇ final layer's output\n",
        "        target_activations, output = self.feature_extractor(x)\n",
        "        print('target_activations[0].size: {}'.format(target_activations[0].size()))# for vgg'35 ([1, 512, 14, 14])\n",
        "        print('output.size: {}'.format(output.size()))                              # for vgg'36 ([1, 512, 7, 7])\n",
        "\n",
        "        for i, classifier in enumerate(self.classifier_block):\n",
        "            if i == len(self.classifier_block) - 1:\n",
        "                output = output.view(output.size(0), -1)\n",
        "                print('output.view.size: {}'.format(output.size()))                 # for vgg'36 ([1, 25088])\n",
        "\n",
        "            output = getattr(self.model, classifier)(output)\n",
        "            print('output.size: {}'.format(output.size()))                          # for vgg'36 ([1, 1000])\n",
        "\n",
        "        return target_activations, output\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    preprocessed_img = img.copy()[:, :, ::-1]\n",
        "    for i in range(3):\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "    preprocessed_img = \\\n",
        "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "    preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "    preprocessed_img.unsqueeze_(0)\n",
        "    input = preprocessed_img.requires_grad_(True)\n",
        "    return input\n",
        "\n",
        "\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
        "\n",
        "\n",
        "class GradCam:\n",
        "    def __init__(self, model, pre_feature_block, feature_block, target_layer_names, classifier_block, use_cuda):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.pre_feature_block = pre_feature_block\n",
        "        self.feature_block = feature_block\n",
        "        self.classifier_block = classifier_block\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "\n",
        "        self.extractor = ModelOutputs(self.model,\n",
        "                                      pre_feature_block,\n",
        "                                      feature_block,\n",
        "                                      target_layer_names,\n",
        "                                      classifier_block)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "    def __call__(self, input, index=None):\n",
        "        if self.cuda:\n",
        "            features, output = self.extractor(input.cuda())\n",
        "        else:\n",
        "            features, output = self.extractor(input)\n",
        "\n",
        "        if index == None:\n",
        "            index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "        with open('/content/gdrive/My Drive/DeepLearning/Face_detection/imagenet_class_index.json') as f:\n",
        "            labels = json.load(f)\n",
        "        \n",
        "        print('prediction[{}]: {}'.format(index, labels[str(index)][1]))\n",
        "        print('output.size: {}'.format(output.size()))                            # for vgg'36 ([1, 1000])\n",
        "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0][index] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        if self.cuda:\n",
        "            one_hot = torch.sum(one_hot.cuda() * output)\n",
        "        else:\n",
        "            one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "        #print('output: {}'.format(output))\n",
        "        #print('one_hot: {}'.format(one_hot))                                      # \n",
        "        getattr(self.model, self.feature_block).zero_grad()\n",
        "        for classifier in self.classifier_block:\n",
        "            getattr(self.model, classifier).zero_grad()\n",
        "        one_hot.backward(retain_graph=True)\n",
        "\n",
        "        gradients = self.extractor.get_gradients()\n",
        "        #print('len(gradients): {}'.format(len(gradients)))\n",
        "        print('gradients[0].size(): {}'.format(gradients[0].size()))\n",
        "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
        "\n",
        "        target = features[-1]\n",
        "        print('target.size(): {}'.format(target.size()))\n",
        "        target = target.cpu().data.numpy()[0, :]\n",
        "        print('target.shape: {}'.format(target.shape))\n",
        "\n",
        "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
        "        print('weights.shape: {}'.format(weights.shape))\n",
        "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
        "        print('cam.shape: {}'.format(cam.shape))            # (14, 14)\n",
        "\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * target[i, :, :]\n",
        "        \n",
        "        #print('cam: {}'.format(cam))\n",
        "        print('cam.shape: {}'.format(cam.shape))\n",
        "        cam = np.maximum(cam, 0)                            # remove negative numbers\n",
        "        cam = cv2.resize(cam, (224, 224))\n",
        "        print('cam.shape: {}'.format(cam.shape))\n",
        "        #print('cam: {}'.format(cam))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "\n",
        "class GuidedBackpropReLU(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(self, input):\n",
        "        positive_mask = (input > 0).type_as(input)\n",
        "        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
        "        self.save_for_backward(input, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        input, output = self.saved_tensors\n",
        "        grad_input = None\n",
        "\n",
        "        positive_mask_1 = (input > 0).type_as(grad_output)\n",
        "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
        "        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),\n",
        "                                   torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,\n",
        "                                                 positive_mask_1), positive_mask_2)\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class GuidedBackpropReLUModel:\n",
        "    def __init__(self, model, use_cuda):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "\n",
        "        # replace ReLU with GuidedBackpropReLU\n",
        "        for idx, module in self.model.features._modules.items():\n",
        "            if module.__class__.__name__ == 'ReLU':\n",
        "                self.model.features._modules[idx] = GuidedBackpropReLU.apply\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "    def __call__(self, input, index=None):\n",
        "        if self.cuda:\n",
        "            output = self.forward(input.cuda())\n",
        "        else:\n",
        "            output = self.forward(input)\n",
        "\n",
        "        if index == None:\n",
        "            index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0][index] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        if self.cuda:\n",
        "            one_hot = torch.sum(one_hot.cuda() * output)\n",
        "        else:\n",
        "            one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "        one_hot.backward(retain_graph=True)\n",
        "\n",
        "        output = input.grad.cpu().data.numpy()\n",
        "        output = output[0, :, :, :]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--use-cuda', action='store_true', default=False,\n",
        "                        help='Use NVIDIA GPU acceleration')\n",
        "    parser.add_argument('--image-path', type=str, default='./examples/both.png',\n",
        "                        help='Input image path')\n",
        "    args = parser.parse_args()\n",
        "    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
        "    if args.use_cuda:\n",
        "        print(\"Using GPU for acceleration\")\n",
        "    else:\n",
        "        print(\"Using CPU for computation\")\n",
        "\n",
        "    return args\n",
        "\n",
        "def deprocess_image(img):\n",
        "    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
        "    img = img - np.mean(img)\n",
        "    img = img / (np.std(img) + 1e-5)\n",
        "    img = img * 0.1\n",
        "    img = img + 0.5\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return np.uint8(img*255)\n",
        "\n",
        "###########grad-cam ends\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    # model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=True, use_pretrained=True)\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # for v in range(10):\n",
        "    #   image_list = []\n",
        "\n",
        "    #   image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "\n",
        "\n",
        "    #   for i in range(11):\n",
        "    #     images = image_id+ '_'+ str(i) + '.jpg'\n",
        "        \n",
        "          \n",
        "    #     source_file = images\n",
        "\n",
        "    #     frame = cv2.imread(source_file)\n",
        "    #     frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "\n",
        "    #     parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "\n",
        "    #     cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f_bw_res/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    #     cv2_imshow(frame)\n",
        "    #     # cv2_imshow('Face Detector', frame)\n",
        "    #     key = cv2.waitKey(0) & 0x00\n",
        "    #     if key == ord(\"q\"):\n",
        "    #       cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "    # model = model_utk\n",
        "    # model = models.resnet50(pretrained=True)\n",
        "    # grad_cam = GradCam(model=model, feature_module=model.layer4, \\\n",
        "    #                    target_layer_names=[\"2\"], use_cuda=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # img = cv2.imread(\"/content/gdrive/My Drive/DeepLearning/Face_detection/test/cui.jpg\", 1)\n",
        "    # img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "    # input = preprocess_image(img)\n",
        "\n",
        "    # # If None, returns the map for the highest scoring category.\n",
        "    # # Otherwise, targets the requested index.\n",
        "    # target_index = None\n",
        "    # mask = grad_cam(input, target_index)\n",
        "\n",
        "    # show_cam_on_image(img, mask)\n",
        "\n",
        "    # gb_model = GuidedBackpropReLUModel(model=model, use_cuda= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # print(model._modules.items())\n",
        "    # gb = gb_model(input, index=target_index)\n",
        "    # gb = gb.transpose((1, 2, 0))\n",
        "    # cam_mask = cv2.merge([mask, mask, mask])\n",
        "    # cam_gb = deprocess_image(cam_mask*gb)\n",
        "    # gb = deprocess_image(gb)\n",
        "\n",
        "    # cv2.imwrite('gb.jpg', gb)\n",
        "    # cv2.imwrite('cam_gb.jpg', cam_gb)\n",
        "    # cv2_imshow(cam_gb) \n",
        "    # cv2_imshow(gb) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    image_path = \"/content/gdrive/My Drive/DeepLearning/Face_detection/test/yutsuru.jpg\"\n",
        "    use_cuda = True\n",
        "\n",
        "    # Can work with any model, but it assumes that the model has a\n",
        "    # feature method, and a classifier method,\n",
        "    # as in the VGG models in torchvision.\n",
        "    config = {\n",
        "        'vgg19':    {\n",
        "            'pre_feature': [],\n",
        "            'features': 'features',\n",
        "            'target': ['35'],\n",
        "            'classifier': ['classifier']\n",
        "        }, \n",
        "        'resnet50': {\n",
        "            'pre_feature': ['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3'],\n",
        "            'features': 'layer4',\n",
        "            'target': ['2'],\n",
        "            'classifier': ['avgpool', 'fc']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    model_name = 'resnet50'\n",
        "    config = config[model_name]\n",
        "    model = getattr(models, model_name)(pretrained=True)\n",
        "    grad_cam = GradCam(model,\n",
        "                       pre_feature_block=config['pre_feature'],\n",
        "                       feature_block=config['features'], # features\n",
        "                       target_layer_names=config['target'],\n",
        "                       classifier_block=config['classifier'],  # classifier\n",
        "                       use_cuda=use_cuda)\n",
        "\n",
        "    img = cv2.imread(image_path, 1)\n",
        "    img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "\n",
        "    #print('img.size(): {}'.format(img.size))\n",
        "    input = preprocess_image(img)\n",
        "    #print('input.size(): {}'.format(input.size()))\n",
        "\n",
        "    # If None, returns the map for the highest scoring category.\n",
        "    # Otherwise, targets the requested index.\n",
        "    target_index = None\n",
        "    mask = grad_cam(input, target_index)\n",
        "    \n",
        "    show_cam_on_image(img, mask)\n",
        "\n",
        "\n",
        "    # gb_model = GuidedBackpropReLUModel(model=model_utk, use_cuda=use_cuda)\n",
        "    # gb = gb_model(input, index=target_index)\n",
        "    # gb = gb.transpose((1, 2, 0))\n",
        "    # cam_mask = cv2.merge([mask, mask, mask])\n",
        "    # cam_gb = deprocess_image(cam_mask*gb)\n",
        "    # gb = deprocess_image(gb)\n",
        "    # cv2.imwrite('gb.jpg', gb)\n",
        "    # cv2.imwrite('cam_gb.jpg', cam_gb)\n",
        "\n",
        "    # cv2_imshow(cam_gb) \n",
        "    # cv2_imshow(gb) \n",
        "\n",
        "    '''\n",
        "    gb_model = GuidedBackpropReLUModel(model=models.vgg19(pretrained=True), use_cuda=use_cuda)\n",
        "    gb = gb_model(input, index=target_index)\n",
        "    gb = gb.transpose((1, 2, 0))\n",
        "    cam_mask = cv2.merge([mask, mask, mask])\n",
        "    cam_gb = deprocess_image(cam_mask*gb)\n",
        "    gb = deprocess_image(gb)\n",
        "    cv2.imwrite('gb.jpg', gb)\n",
        "    cv2.imwrite('cam_gb.jpg', cam_gb)\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnoyW3v_mfs7"
      },
      "source": [
        "import imutils\n",
        "import cv2\n",
        "import torch\n",
        "from vision_utils.custom_architectures import PretrainedMT, SepConvModel, SepConvModelMT\n",
        "from vision_utils.custom_torch_utils import initialize_model\n",
        "from emotion_detection.evaluate import predict_fer\n",
        "from multitask_rag.evaluate import predict_utk\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams.update({'figure.autolayout': True})\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageOps\n",
        "from torch.autograd import Function\n",
        "from torchvision import models\n",
        "import json\n",
        "\n",
        "\n",
        "# default path to a saved model for race, age and gender prediction\n",
        "# saved_weight_utk = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'multitask_rag/checkpoints/vgg_model_21_val_loss=4.139335.pth'\n",
        "\n",
        "\n",
        "# saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.217711824612125.pth\"\n",
        "saved_weight_utk =\"/content/gdrive/My Drive/DeepLearning/Face_detection/checkpoints/resnet_adam/resnet_checkpoint_val_loss=-6.249345162580937.pth\"\n",
        "\n",
        "# default path to a saved model for emotion prediction\n",
        "# saved_weight_fer = '/media/sf_Documents/COMPUTER_VISION/DEmoClassi/' \\\n",
        "#                    'emotion_detection/checkpoints/vgg_model_173_val_accuracy=0.6447478.pth'\n",
        "\n",
        "\n",
        "# paths to the caffe model files for detecting faces using opencv\n",
        "# package_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "package_path = os.path.dirname(os.path.dirname(os.path.abspath('/content/gdrive/My Drive/DEmoClassi-master/cv2_dnn_model_files')))\n",
        "path_binaries = os.path.join(package_path, 'cv2_dnn_model_files')\n",
        "path_caffe_model = os.path.join(path_binaries, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
        "path_proto = os.path.join(path_binaries, 'deploy.prototxt.txt')\n",
        "\n",
        "def dict_prob_to_list(dict_probs):\n",
        "    \"\"\"\n",
        "    utility function for converting a dictionary of labels with their probabilities\n",
        "     into two lists of labels and probs resp.\n",
        "    \"\"\"\n",
        "    items = list(dict_probs.items())\n",
        "    return [item[0] for item in items], [item[1] for item in items]\n",
        "\n",
        "\n",
        "def plot_to_array(x, y, color):\n",
        "    \"\"\"Utility function for ploting predicted probabilities as bar plots\"\"\"\n",
        "    fig = plt.figure(figsize=(2, 2))\n",
        "    fig.add_subplot(111)\n",
        "    # fig.tight_layout(pad=0)\n",
        "    plt.barh(x, y, color=color)\n",
        "    fig.canvas.draw()\n",
        "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "    plt.clf()\n",
        "    return data\n",
        "\n",
        "def evaluate_model(model, dataloader,\n",
        "                   title='Confusion matrix',\n",
        "                   labels_=[[0, 1], [0, 1, 2, 3, 4, 5,6 ]],\n",
        "                   target_names=[['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]],\n",
        "                   normalize=False):\n",
        "    \"\"\"\n",
        "        Function for evaluating a classification model by printing/plotting classification report and confusion matrix\n",
        "        :param model: a pytorch trained model\n",
        "        :param dataloader: a pytorch DataLoader object, or any object that yields pytorch tensors\n",
        "                ready to be used by the model\n",
        "        :param title: a string to be used as the plot title\n",
        "        :param labels_: list  of lists , each sublist is a list of integers (0 to number of classes - 1) representing\n",
        "                        labels for an output from the model\n",
        "        :param target_names: list of lists, each sublist is a list of strings or ints that describe the labels,\n",
        "                            and must have the same length as the corresponding labels it describes from `labels`list\n",
        "        :param normalize: whether to show the actual values or in % for the confusion matrix\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    y_age = []\n",
        "    y_gender = []\n",
        "    y_race = []\n",
        "    y_pred_age = []\n",
        "    y_pred_gender = []\n",
        "    y_pred_race = []\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # first, get the predictions\n",
        "    model.eval()  # set model in evaluation mode\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over data.\n",
        "        for inputs, age, gender, race in tqdm.tqdm(dataloader):\n",
        "            inputs = inputs.to(device, dtype=torch.float32)\n",
        "            y_age.append(age)\n",
        "            y_gender.append(gender)\n",
        "            y_race.append(race)\n",
        "\n",
        "            age_pred, gender_pred, race_pred = model(inputs)\n",
        "            y_pred_age.append(age_pred.to('cpu').numpy())\n",
        "            _, gender_pred = torch.max(gender_pred, 1)\n",
        "            _, race_pred = torch.max(race_pred, 1)\n",
        "            y_pred_gender.append(gender_pred.to('cpu').numpy())\n",
        "            y_pred_race.append(race_pred.to('cpu').numpy())\n",
        "\n",
        "    # print classification report\n",
        "    y_age, y_pred_age = np.concatenate(y_age), np.concatenate(y_pred_age)\n",
        "    y_gender, y_pred_gender = np.concatenate(y_gender), np.concatenate(y_pred_gender)\n",
        "    y_race, y_pred_race = np.concatenate(y_race), np.concatenate(y_pred_race)\n",
        "\n",
        "    print('----------------------- Age prediction -------------------------')\n",
        "    print(f\"Mean Absolute Error {np.abs(y_age - y_pred_age).mean():.4f}\")\n",
        "\n",
        "    print('----------------------- Gender prediction -------------------------')\n",
        "    plot_confusion_matrix(y_gender, y_pred_gender, title, labels_[0], target_names[0], normalize)\n",
        "\n",
        "    print('----------------------- Race prediction -------------------------')\n",
        "    plot_confusion_matrix(y_race, y_pred_race, title, labels_[1], target_names[1], normalize)\n",
        "\n",
        "\n",
        "def preprocess_utk(image):\n",
        "    transf = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return transf(image).unsqueeze_(0)\n",
        "\n",
        "\n",
        "def predict_utk(image, model):\n",
        "\n",
        "    # process image\n",
        "    image = preprocess_utk(image)\n",
        "\n",
        "    # prepare model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # predict probabilities\n",
        "    age_pred, gender_pred, race_pred = model(image)\n",
        "    age_pred = age_pred.detach().to('cpu').numpy()[0][0]\n",
        "    gender_probs, race_probs = F.softmax(gender_pred, dim=1).detach().to('cpu').numpy()[0],\\\n",
        "                               F.softmax(race_pred, dim=1).detach().to('cpu').numpy()[0]\n",
        "\n",
        "    # map probabilities to label names\n",
        "    gender_labs, race_labs = ['Male', 'Female'], ['White', 'Black', 'East Asian', 'Southeast Asian', 'Middle Eastern','Latino_Hispanic', 'Indian' ]\n",
        "    gender_label_pred = gender_labs[np.argmax(gender_probs)]\n",
        "    race_label_pred = race_labs[np.argmax(race_probs)]\n",
        "\n",
        "    gender = dict(zip(gender_labs, gender_probs))\n",
        "    race = dict(zip(race_labs, race_probs))\n",
        "\n",
        "    return age_pred, gender, gender_label_pred, race, race_label_pred\n",
        "\n",
        "\n",
        "def predict_from_frame(net, frame, model_utk, display_probs):\n",
        "    \"\"\"\n",
        "    Makes emotion, gender, age and race prediction from a frame and plot the results in the frame to display\n",
        "     using opencv\n",
        "    :param net: opencv face detector\n",
        "    :param frame: numpy array representing the image from hich to detect face and make prediction\n",
        "    :param model_utk: pytorch model for predicting race, age and gender\n",
        "    # :param model_fer: pytorch model for predicting emotion\n",
        "    #:param transfer_learn: whether we are using a pretrained model (`resnet` or `vgg`)\n",
        "    :param display_probs: True or False, whether to plot the predicted probabilities for each class\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # transfer_learn = True\n",
        "    frame = imutils.resize(frame, width=600, height=600)\n",
        "\n",
        "    # Prepare the opencv face detector\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # iterate through the detected faces\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # if the model has detected face with at least 50% confidence\n",
        "        # get the bounding box of the face and plot it\n",
        "        if confidence > 0.5:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            # cv2.rectangle(frame, (startX - 25, startY - 50), (endX + 25, endY + 25), (0, 255, 0), 1)\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
        "\n",
        "            face = frame[startY: endY, startX: endX]\n",
        "            age, gender, gender_lab, race, race_lab = predict_utk(face, model_utk)\n",
        "\n",
        "  \n",
        "            try:\n",
        "                if race_lab == \"Latino_Hispanic\":\n",
        "                  race_lab = \"L/H\"\n",
        "                if race_lab == \"East Asian\":\n",
        "                  race_lab = \"E Asian\"\n",
        "                if race_lab == \"Southeast Asian\":\n",
        "                  race_lab = \"SE Asian\"\n",
        "                if race_lab == \"Middle Eastern\":\n",
        "                  race_lab = \"ME\"\n",
        "\n",
        "\n",
        "                if gender_lab == \"Male\":\n",
        "                  gender_lab = \"M\"\n",
        "                if gender_lab == \"Female\":\n",
        "                  gender_lab = \"F\"\n",
        "\n",
        "                gender_labels, gender_proba = dict_prob_to_list(gender)\n",
        "                race_labels,  race_proba = dict_prob_to_list(race)\n",
        "\n",
        "######b/w version             \n",
        "                if race_proba[0] > race_proba[1]:\n",
        "                  white_prob = round(race_proba[0],3)\n",
        "                  race_text = f\"{'White', white_prob}\"\n",
        "                else: \n",
        "                  race_lab = race_labels[1]                  \n",
        "                  black_prob = round(race_proba[1],3)\n",
        "                  race_text = f\"{'Black', black_prob}\"\n",
        "######b/w version\n",
        "                # race_text = f\"{'test', round(max(race_proba),3)}\"\n",
        "                gender_text = f\"{gender_lab, round(max(gender_proba),3)}\"\n",
        "                print(race_text, gender_text)\n",
        "\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(frame, race_text, (startX , startY + 12), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "                cv2.putText(frame, gender_text, (startX , endY - 2), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "            except:\n",
        "                pass\n",
        "    return frame\n",
        "    print('ojbk')\n",
        "    print(text)\n",
        "\n",
        "##########grad-cam starts\n",
        "\n",
        "# class FeatureExtractor():\n",
        "#     \"\"\" Class for extracting activations and \n",
        "#     registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "#     def __init__(self, model, target_layers):\n",
        "#         self.model = model\n",
        "#         self.target_layers = target_layers\n",
        "#         self.gradients = []\n",
        "\n",
        "#     def save_gradient(self, grad):\n",
        "#         self.gradients.append(grad)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         outputs = []\n",
        "#         self.gradients = []\n",
        "#         for name, module in self.model._modules.items():\n",
        "#             x = module(x)\n",
        "#             if name in self.target_layers:\n",
        "#                 x.register_hook(self.save_gradient)\n",
        "#                 outputs += [x]\n",
        "#         return outputs, x\n",
        "\n",
        "\n",
        "# class ModelOutputs():\n",
        "#     \"\"\" Class for making a forward pass, and getting:\n",
        "#     1. The network output.\n",
        "#     2. Activations from intermeddiate targetted layers.\n",
        "#     3. Gradients from intermeddiate targetted layers. \"\"\"\n",
        "\n",
        "#     def __init__(self, model, feature_module, target_layers):\n",
        "#         self.model = model\n",
        "#         self.feature_module = feature_module\n",
        "#         self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)\n",
        "\n",
        "#     def get_gradients(self):\n",
        "#         return self.feature_extractor.gradients\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         target_activations = []\n",
        "#         for name, module in self.model._modules.items():\n",
        "#             if module == self.feature_module:\n",
        "#                 target_activations, x = self.feature_extractor(x)\n",
        "#             elif \"avgpool\" in name.lower():\n",
        "#                 x = module(x)\n",
        "#                 x = x.view(x.size(0),-1)\n",
        "#             else:\n",
        "#                 x = module(x)\n",
        "        \n",
        "#         return target_activations, x\n",
        "\n",
        "\n",
        "# def preprocess_image(img):\n",
        "#     means = [0.485, 0.456, 0.406]\n",
        "#     stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "#     preprocessed_img = img.copy()[:, :, ::-1]\n",
        "#     for i in range(3):\n",
        "#         preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "#         preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "#     preprocessed_img = \\\n",
        "#         np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "#     preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "#     preprocessed_img.unsqueeze_(0)\n",
        "#     input = preprocessed_img.requires_grad_(True)\n",
        "#     return input\n",
        "\n",
        "\n",
        "# def show_cam_on_image(img, mask):\n",
        "#     heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "#     heatmap = np.float32(heatmap) / 255\n",
        "#     cam = heatmap + np.float32(img)\n",
        "#     cam = cam / np.max(cam)\n",
        "#     cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
        "\n",
        "\n",
        "# class GradCam:\n",
        "#     def __init__(self, model, feature_module, target_layer_names, use_cuda):\n",
        "#         self.model = model\n",
        "#         self.feature_module = feature_module\n",
        "#         self.model.eval()\n",
        "#         self.cuda = use_cuda\n",
        "#         if self.cuda:\n",
        "#             self.model = model.cuda()\n",
        "\n",
        "#         self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self.model(input)\n",
        "\n",
        "#     def __call__(self, input, index=None):\n",
        "#         if self.cuda:\n",
        "#             features, output = self.extractor(input.cuda())\n",
        "#         else:\n",
        "#             features, output = self.extractor(input)\n",
        "\n",
        "#         if index == None:\n",
        "#             index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "#         one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "#         one_hot[0][index] = 1\n",
        "#         one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "#         if self.cuda:\n",
        "#             one_hot = torch.sum(one_hot.cuda() * output)\n",
        "#         else:\n",
        "#             one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "#         self.feature_module.zero_grad()\n",
        "#         self.model.zero_grad()\n",
        "#         one_hot.backward(retain_graph=True)\n",
        "\n",
        "#         grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
        "\n",
        "#         target = features[-1]\n",
        "#         target = target.cpu().data.numpy()[0, :]\n",
        "\n",
        "#         weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
        "#         cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
        "\n",
        "#         for i, w in enumerate(weights):\n",
        "#             cam += w * target[i, :, :]\n",
        "\n",
        "#         cam = np.maximum(cam, 0)\n",
        "#         cam = cv2.resize(cam, input.shape[2:])\n",
        "#         cam = cam - np.min(cam)\n",
        "#         cam = cam / np.max(cam)\n",
        "#         return cam\n",
        "\n",
        "\n",
        "# class GuidedBackpropReLU(Function):\n",
        "\n",
        "#     @staticmethod\n",
        "#     def forward(self, input):\n",
        "#         positive_mask = (input > 0).type_as(input)\n",
        "#         output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
        "#         self.save_for_backward(input, output)\n",
        "#         return output\n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(self, grad_output):\n",
        "#         input, output = self.saved_tensors\n",
        "#         grad_input = None\n",
        "\n",
        "#         positive_mask_1 = (input > 0).type_as(grad_output)\n",
        "#         positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
        "#         grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),\n",
        "#                                    torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,\n",
        "#                                                  positive_mask_1), positive_mask_2)\n",
        "\n",
        "#         return grad_input\n",
        "\n",
        "\n",
        "# class GuidedBackpropReLUModel:\n",
        "#     def __init__(self, model, use_cuda):\n",
        "#         self.model = model\n",
        "#         self.model.eval()\n",
        "#         self.cuda = use_cuda\n",
        "#         if self.cuda:\n",
        "#             self.model = model.cuda()\n",
        "\n",
        "#         def recursive_relu_apply(module_top):\n",
        "#             for idx, module in module_top._modules.items():\n",
        "#                 recursive_relu_apply(module)\n",
        "#                 if module.__class__.__name__ == 'ReLU':\n",
        "#                     module_top._modules[idx] = GuidedBackpropReLU.apply\n",
        "                \n",
        "#         # replace ReLU with GuidedBackpropReLU\n",
        "#         recursive_relu_apply(self.model)\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self.model(input)\n",
        "\n",
        "#     def __call__(self, input, index=None):\n",
        "#         if self.cuda:\n",
        "#             output = self.forward(input.cuda())\n",
        "#         else:\n",
        "#             output = self.forward(input)\n",
        "\n",
        "#         if index == None:\n",
        "#             index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "#         one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "#         one_hot[0][index] = 1\n",
        "#         one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "#         if self.cuda:\n",
        "#             one_hot = torch.sum(one_hot.cuda() * output)\n",
        "#         else:\n",
        "#             one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "#         # self.model.features.zero_grad()\n",
        "#         # self.model.classifier.zero_grad()\n",
        "#         one_hot.backward(retain_graph=True)\n",
        "\n",
        "#         output = input.grad.cpu().data.numpy()\n",
        "#         output = output[0, :, :, :]\n",
        "\n",
        "#         return output\n",
        "\n",
        "\n",
        "# def get_args():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--use-cuda', action='store_true', default=True,\n",
        "#                         help='Use NVIDIA GPU acceleration')\n",
        "#     parser.add_argument('--image-path', type=str, default='./examples/both.png',\n",
        "#                         help='Input image path')\n",
        "#     args = parser.parse_args()\n",
        "#     args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
        "#     if args.use_cuda:\n",
        "#         print(\"Using GPU for acceleration\")\n",
        "#     else:\n",
        "#         print(\"Using CPU for computation\")\n",
        "\n",
        "#     return args\n",
        "\n",
        "# def deprocess_image(img):\n",
        "#     \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
        "#     img = img - np.mean(img)\n",
        "#     img = img / (np.std(img) + 1e-5)\n",
        "#     img = img * 0.1\n",
        "#     img = img + 0.5\n",
        "#     img = np.clip(img, 0, 1)\n",
        "#     return np.uint8(img*255)\n",
        "###########grad-cam ends\n",
        "\n",
        "\n",
        "\n",
        "###########grad-cam starts\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Function\n",
        "from torchvision import models\n",
        "from torchvision import utils\n",
        "import cv2\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import torch.nn as nn\n",
        "i=0##testing in what\n",
        "resnet = models.resnet50(pretrained=True)#这里单独加载一个包含全连接层的resnet50模型\n",
        "image = []\n",
        "class FeatureExtractor():\n",
        "    \"\"\" Class for extracting activations and \n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "    def __init__(self, model, target_layers):\n",
        "        self.model = model\n",
        "        self.target_layers = target_layers\n",
        "        self.gradients = []\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "    \tself.gradients.append(grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outputs = []\n",
        "        self.gradients = []\n",
        "        for name, module in self.model._modules.items():##resnet50没有.feature这个特征，直接删除用就可以。\n",
        "            x = module(x)\n",
        "            #print('name=',name)\n",
        "            #print('x.size()=',x.size())\n",
        "            if name in self.target_layers:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                outputs += [x]\n",
        "            #print('outputs.size()=',x.size())\n",
        "        #print('len(outputs)',len(outputs))\n",
        "        return outputs, x\n",
        "\n",
        "class ModelOutputs():\n",
        "\t\"\"\" Class for making a forward pass, and getting:\n",
        "\t1. The network output.\n",
        "\t2. Activations from intermeddiate targetted layers.\n",
        "\t3. Gradients from intermeddiate targetted layers. \"\"\"\n",
        "\tdef __init__(self, model, target_layers,use_cuda):\n",
        "\t\tself.model = model\n",
        "\t\tself.feature_extractor = FeatureExtractor(self.model, target_layers)\n",
        "\t\tself.cuda = use_cuda\n",
        "\tdef get_gradients(self):\n",
        "\t\treturn self.feature_extractor.gradients\n",
        "\n",
        "\tdef __call__(self, x):\n",
        "\t\ttarget_activations, output  = self.feature_extractor(x)\n",
        "\t\toutput = output.view(output.size(0), -1)\n",
        "\t\t#print('classfier=',output.size())\n",
        "\t\tif self.cuda:\n",
        "\t\t\toutput = output.cpu()\n",
        "\t\t\toutput = resnet.fc(output).cuda()##这里就是为什么我们多加载一个resnet模型进来的原因，因为后面我们命名的model不包含fc层，但是这里又偏偏要使用。#\n",
        "\t\telse:\n",
        "\t\t\toutput = resnet.fc(output)##这里对应use-cuda上更正一些bug,不然用use-cuda的时候会导致类型对不上,这样保证既可以在cpu上运行,gpu上运行也不会出问题.\n",
        "\t\treturn target_activations, output\n",
        "\n",
        "def preprocess_image(img):\n",
        "\tmeans=[0.485, 0.456, 0.406]\n",
        "\tstds=[0.229, 0.224, 0.225]\n",
        "\n",
        "\tpreprocessed_img = img.copy()[: , :, ::-1]\n",
        "\tfor i in range(3):\n",
        "\t\tpreprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "\t\tpreprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "\tpreprocessed_img = \\\n",
        "\t\tnp.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "\tpreprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "\tpreprocessed_img.unsqueeze_(0)\n",
        "\tinput = preprocessed_img\n",
        "\tinput.requires_grad = True\n",
        "\treturn input\n",
        "\n",
        "def show_cam_on_image(img, mask,name):\n",
        "\theatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
        "\theatmap = np.float32(heatmap) / 255\n",
        "\tcam = heatmap + np.float32(img)\n",
        "\tcam = cam / np.max(cam)\n",
        "\tcv2.imwrite(\"cam/cam_{}.jpg\".format(name), np.uint8(255 * cam))\n",
        "class GradCam:\n",
        "\tdef __init__(self, model, target_layer_names, use_cuda):\n",
        "\t\tself.model = model\n",
        "\t\tself.model.eval()\n",
        "\t\tself.cuda = use_cuda\n",
        "\t\tif self.cuda:\n",
        "\t\t\tself.model = model.cuda()\n",
        "\n",
        "\t\tself.extractor = ModelOutputs(self.model, target_layer_names, use_cuda)\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\treturn self.model(input) \n",
        "\n",
        "\tdef __call__(self, input, index = None):\n",
        "\t\tif self.cuda:\n",
        "\t\t\tfeatures, output = self.extractor(input.cuda())\n",
        "\t\telse:\n",
        "\t\t\tfeatures, output = self.extractor(input)\n",
        "\n",
        "\t\tif index == None:\n",
        "\t\t\tindex = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "\t\tone_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
        "\t\tone_hot[0][index] = 1\n",
        "\t\tone_hot = torch.Tensor(torch.from_numpy(one_hot))\n",
        "\t\tone_hot.requires_grad = True\n",
        "\t\tif self.cuda:\n",
        "\t\t\tone_hot = torch.sum(one_hot.cuda() * output)\n",
        "\t\telse:\n",
        "\t\t\tone_hot = torch.sum(one_hot * output)\n",
        "\n",
        "\t\tself.model.zero_grad()##features和classifier不包含，可以重新加回去试一试，会报错不包含这个对象。\n",
        "\t\t#self.model.zero_grad()\n",
        "\t\tone_hot.backward(retain_graph=True)##这里适配我们的torch0.4及以上，我用的1.0也可以完美兼容。（variable改成graph即可）\n",
        "\n",
        "\t\tgrads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
        "\t\t#print('grads_val',grads_val.shape)\n",
        "\t\ttarget = features[-1]\n",
        "\t\ttarget = target.cpu().data.numpy()[0, :]\n",
        "\n",
        "\t\tweights = np.mean(grads_val, axis = (2, 3))[0, :]\n",
        "\t\t#print('weights',weights.shape)\n",
        "\t\tcam = np.zeros(target.shape[1 : ], dtype = np.float32)\n",
        "\t\t#print('cam',cam.shape)\n",
        "\t\t#print('features',features[-1].shape)\n",
        "\t\t#print('target',target.shape)\n",
        "\t\tfor i, w in enumerate(weights):\n",
        "\t\t\tcam += w * target[i, :, :]\n",
        "\n",
        "\t\tcam = np.maximum(cam, 0)\n",
        "\t\tcam = cv2.resize(cam, (224, 224))\n",
        "\t\tcam = cam - np.min(cam)\n",
        "\t\tcam = cam / np.max(cam)\n",
        "\t\treturn cam\n",
        "class GuidedBackpropReLUModel:\n",
        "\tdef __init__(self, model, use_cuda):\n",
        "\t\tself.model = model#这里同理，要的是一个完整的网络，不然最后维度会不匹配。\n",
        "\t\tself.model.eval()\n",
        "\t\tself.cuda = use_cuda\n",
        "\t\tif self.cuda:\n",
        "\t\t\tself.model = model.cuda()\n",
        "\t\tfor module in self.model.named_modules():\n",
        "\t\t\tmodule[1].register_backward_hook(self.bp_relu)\n",
        "\n",
        "\tdef bp_relu(self, module, grad_in, grad_out):\n",
        "\t\tif isinstance(module, nn.ReLU):\n",
        "\t\t\treturn (torch.clamp(grad_in[0], min=0.0),)\n",
        "\tdef forward(self, input):\n",
        "\t\treturn self.model(input)\n",
        "\n",
        "\tdef __call__(self, input, index = None):\n",
        "\t\tif self.cuda:\n",
        "\t\t\toutput = self.forward(input.cuda())\n",
        "\t\telse:\n",
        "\t\t\toutput = self.forward(input)\n",
        "\t\tif index == None:\n",
        "\t\t\tindex = np.argmax(output.cpu().data.numpy())\n",
        "\t\t#print(input.grad)\n",
        "\t\tone_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
        "\t\tone_hot[0][index] = 1\n",
        "\t\tone_hot = torch.from_numpy(one_hot)\n",
        "\t\tone_hot.requires_grad = True\n",
        "\t\tif self.cuda:\n",
        "\t\t\tone_hot = torch.sum(one_hot.cuda() * output)\n",
        "\t\telse:\n",
        "\t\t\tone_hot = torch.sum(one_hot * output)\n",
        "\t\t#self.model.classifier.zero_grad()\n",
        "\t\tone_hot.backward(retain_graph=True)\n",
        "\t\toutput = input.grad.cpu().data.numpy()\n",
        "\t\toutput = output[0,:,:,:]\n",
        "\n",
        "\t\treturn output\n",
        "\n",
        "def get_args():\n",
        "\tparser = argparse.ArgumentParser()\n",
        "\tparser.add_argument('--use-cuda', action='store_true', default=False,\n",
        "\t                    help='Use NVIDIA GPU acceleration')\n",
        "\tparser.add_argument('--image-path', type=str, default='./examples/',\n",
        "\t                    help='Input image path')\n",
        "\targs = parser.parse_args()\n",
        "\targs.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
        "\tif args.use_cuda:\n",
        "\t    print(\"Using GPU for acceleration\")\n",
        "\telse:\n",
        "\t    print(\"Using CPU for computation\")\n",
        "\n",
        "\treturn args\n",
        "\n",
        "###########grad-cam ends\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvModelMultiTask(nn.Module):\n",
        "    \"\"\"custom Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(ConvModelMultiTask, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, 'utk', use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "    my_model = ConvModelMultiTask()\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.Adam(\n",
        "        [\n",
        "            {\"params\": my_model.conv_base.fc.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_age.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_gender.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.output_race.parameters(), \"lr\": 1e-3},\n",
        "            {\"params\": my_model.conv_base.conv1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer1.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer2.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer3.parameters()},\n",
        "            {\"params\": my_model.conv_base.layer4.parameters()},\n",
        "        ],\n",
        "        lr=1e-6,\n",
        "    )\n",
        "\n",
        "class PretrainedMT1(nn.Module):\n",
        "    \"\"\"Pretrained Pytorch neural network module for multitask learning\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', feature_extract=True, use_pretrained=True):\n",
        "        super(PretrainedMT1, self).__init__()\n",
        "        self.conv_base, input_size = initialize_model(model_name, feature_extract, num_classes=None,\n",
        "                                                      task='utk', use_pretrained=use_pretrained)\n",
        "        self.output_age = nn.Linear(128, 1)\n",
        "        self.output_gender = nn.Linear(128, 2)\n",
        "        self.output_race = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_base(x)\n",
        "        age = self.output_age(x)\n",
        "        gender = self.output_gender(x)\n",
        "        race = self.output_race(x)\n",
        "        return age, gender, race\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # opencv face detector\n",
        "    cv2_facenet = cv2.dnn.readNetFromCaffe(path_proto, path_caffe_model)\n",
        "\n",
        "    # start detection and prediction\n",
        "    # main(predict_args, cv2_facenet)\n",
        "\n",
        "    display_probs = True\n",
        "    net = cv2_facenet     \n",
        "    \n",
        "    # model_utk = PretrainedMT1('resnet', feature_extract=False, use_pretrained=False)\n",
        "\n",
        "    model_utk = PretrainedMT1('resnet', feature_extract=True, use_pretrained=True)\n",
        "\n",
        "    model_utk.load_state_dict(torch.load(saved_weight_utk, map_location='cpu')['model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # for v in range(10):\n",
        "    #   image_list = []\n",
        "\n",
        "    #   image_id = '/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f/F'+ str(v)\n",
        "\n",
        "\n",
        "\n",
        "    #   for i in range(11):\n",
        "    #     images = image_id+ '_'+ str(i) + '.jpg'\n",
        "        \n",
        "          \n",
        "    #     source_file = images\n",
        "\n",
        "    #     frame = cv2.imread(source_file)\n",
        "    #     frame = predict_from_frame(net, frame, model_utk, display_probs = True)\n",
        "\n",
        "\n",
        "    #     parent, f_name = str(pathlib.Path(source_file).parent), pathlib.Path(source_file).name\n",
        "\n",
        "\n",
        "    #     cv2.imwrite(os.path.join('/content/gdrive/My Drive/DeepLearning/Face_detection/test/mask_f_bw_res/', f_name+'_predicted.jpg'), frame)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    #     cv2_imshow(frame)\n",
        "    #     # cv2_imshow('Face Detector', frame)\n",
        "    #     key = cv2.waitKey(0) & 0x00\n",
        "    #     if key == ord(\"q\"):\n",
        "    #       cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "    # model = model_utk\n",
        "    # model = models.resnet50(pretrained=True)\n",
        "    # grad_cam = GradCam(model=model, feature_module=model.layer4, \\\n",
        "    #                    target_layer_names=[\"2\"], use_cuda=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # img = cv2.imread(\"/content/gdrive/My Drive/DeepLearning/Face_detection/test/cui.jpg\", 1)\n",
        "    # img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "    # input = preprocess_image(img)\n",
        "\n",
        "    # # If None, returns the map for the highest scoring category.\n",
        "    # # Otherwise, targets the requested index.\n",
        "    # target_index = None\n",
        "    # mask = grad_cam(input, target_index)\n",
        "\n",
        "    # show_cam_on_image(img, mask)\n",
        "\n",
        "    # gb_model = GuidedBackpropReLUModel(model=model, use_cuda= \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # print(model._modules.items())\n",
        "    # gb = gb_model(input, index=target_index)\n",
        "    # gb = gb.transpose((1, 2, 0))\n",
        "    # cam_mask = cv2.merge([mask, mask, mask])\n",
        "    # cam_gb = deprocess_image(cam_mask*gb)\n",
        "    # gb = deprocess_image(gb)\n",
        "\n",
        "    # cv2.imwrite('gb.jpg', gb)\n",
        "    # cv2.imwrite('cam_gb.jpg', cam_gb)\n",
        "    # cv2_imshow(cam_gb) \n",
        "    # cv2_imshow(gb) \n",
        "\n",
        "\n",
        "    args = get_args()\n",
        "\n",
        "    # Can work with any model, but it assumes that the model has a \n",
        "    # feature method, and a classifier method,\n",
        "    # as in the VGG models in torchvision.\n",
        "    model = models.resnet50(pretrained=True)#这里相对vgg19而言我们处理的不一样，这里需要删除fc层，因为后面model用到的时候会用不到fc层，只查到fc层之前的所有层数。\n",
        "    del model.fc\n",
        "    print(model)\n",
        "    #modules = list(resnet.children())[:-1]\n",
        "    #model = torch.nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "\n",
        "    image_path = \"/content/gdrive/My Drive/DeepLearning/Face_detection/test/cui.jpg\"\n",
        "    #print(model)\n",
        "    grad_cam = GradCam(model , \\\n",
        "            target_layer_names = [\"layer4\"], use_cuda=\"cuda\" if torch.cuda.is_available() else \"cpu\")##这里改成layer4也很简单，我把每层name和size都打印出来了，想看哪层自己直接嵌套就可以了。（最后你会在终端看得到name的）\n",
        "    # x=os.walk(args.image_path)\n",
        "\n",
        "    x=os.walk(\"/content/gdrive/My Drive/DeepLearning/Face_detection/test/cui.jpg\")\n",
        "    for root,dirs,filename in x:\n",
        "    #print(type(grad_cam))\n",
        "      print(filename)\n",
        "    for s in filename:\n",
        "\n",
        "          # image.append(cv2.imread(\"/content/gdrive/My Drive/DeepLearning/Face_detection/test/cui.jpg\",1))\n",
        "          # image.append(cv2.imread(args.image_path+s,1))\n",
        "      #img = cv2.imread(filename, 1)\n",
        "      img = cv2.imread(\"/content/gdrive/My Drive/DeepLearning/Face_detection/test/cui.jpg\", 1)\n",
        "      \n",
        "    for img in image:\n",
        "      img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "      input = preprocess_image(img)\n",
        "      input.required_grad = True\n",
        "      print('input.size()=',input.size())\n",
        "    # If None, returns the map for the highest scoring category.\n",
        "    # Otherwise, targets the requested index.\n",
        "      target_index =None\n",
        "\n",
        "      mask = grad_cam(input, target_index)\n",
        "      i=i+1 \n",
        "      show_cam_on_image(img, mask,i)\n",
        "\n",
        "      gb_model = GuidedBackpropReLUModel(model = models.resnet50(pretrained=True), use_cuda=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      gb = gb_model(input, index=target_index)\n",
        "      if not os.path.exists('gb'):\n",
        "        os.mkdir('gb')\n",
        "      if not os.path.exists('camgb'):\n",
        "        os.mkdir('camgb')\n",
        "      utils.save_image(torch.from_numpy(gb), 'gb/gb_{}.jpg'.format(i))\n",
        "      cam_mask = np.zeros(gb.shape)\n",
        "      for j in range(0, gb.shape[0]):\n",
        "        cam_mask[j, :, :] = mask\n",
        "      cam_gb = np.multiply(cam_mask, gb)\n",
        "      utils.save_image(torch.from_numpy(cam_gb), 'camgb/cam_gb_{}.jpg'.format(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyGPgfwl4Sgq"
      },
      "source": [
        "???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi8jscMJvpf2"
      },
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torchvision import models\n",
        "\n",
        "class FeatureExtractor():\n",
        "    \"\"\" Class for extracting activations and \n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layers):\n",
        "        self.model = model\n",
        "        self.target_layers = target_layers\n",
        "        self.gradients = []\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients.append(grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        outputs = []\n",
        "        self.gradients = []\n",
        "        for name, module in self.model._modules.items():\n",
        "            x = module(x)\n",
        "            if name in self.target_layers:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                outputs += [x]\n",
        "        return outputs, x\n",
        "\n",
        "\n",
        "class ModelOutputs():\n",
        "    \"\"\" Class for making a forward pass, and getting:\n",
        "    1. The network output.\n",
        "    2. Activations from intermeddiate targetted layers.\n",
        "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
        "\n",
        "    def __init__(self, model, feature_module, target_layers):\n",
        "        self.model = model\n",
        "        self.feature_module = feature_module\n",
        "        self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)\n",
        "\n",
        "    def get_gradients(self):\n",
        "        return self.feature_extractor.gradients\n",
        "\n",
        "    def __call__(self, x):\n",
        "        target_activations = []\n",
        "        for name, module in self.model._modules.items():\n",
        "            if module == self.feature_module:\n",
        "                target_activations, x = self.feature_extractor(x)\n",
        "            elif \"avgpool\" in name.lower():\n",
        "                x = module(x)\n",
        "                x = x.view(x.size(0),-1)\n",
        "            else:\n",
        "                x = module(x)\n",
        "        \n",
        "        return target_activations, x\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    preprocessed_img = img.copy()[:, :, ::-1]\n",
        "    for i in range(3):\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "    preprocessed_img = \\\n",
        "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "    preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "    preprocessed_img.unsqueeze_(0)\n",
        "    input = preprocessed_img.requires_grad_(True)\n",
        "    return input\n",
        "\n",
        "\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    cv2.imwrite(\"cam.jpg\", np.uint8(255 * cam))\n",
        "\n",
        "\n",
        "class GradCam:\n",
        "    def __init__(self, model, feature_module, target_layer_names, use_cuda):\n",
        "        self.model = model\n",
        "        self.feature_module = feature_module\n",
        "        self.model.eval()\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "\n",
        "        self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "    def __call__(self, input, index=None):\n",
        "        if self.cuda:\n",
        "            features, output = self.extractor(input.cuda())\n",
        "        else:\n",
        "            features, output = self.extractor(input)\n",
        "\n",
        "        if index == None:\n",
        "            index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0][index] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        if self.cuda:\n",
        "            one_hot = torch.sum(one_hot.cuda() * output)\n",
        "        else:\n",
        "            one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "        self.feature_module.zero_grad()\n",
        "        self.model.zero_grad()\n",
        "        one_hot.backward(retain_graph=True)\n",
        "\n",
        "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
        "\n",
        "        target = features[-1]\n",
        "        target = target.cpu().data.numpy()[0, :]\n",
        "\n",
        "        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
        "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
        "\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * target[i, :, :]\n",
        "\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, input.shape[2:])\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "\n",
        "class GuidedBackpropReLU(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(self, input):\n",
        "        positive_mask = (input > 0).type_as(input)\n",
        "        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
        "        self.save_for_backward(input, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        input, output = self.saved_tensors\n",
        "        grad_input = None\n",
        "\n",
        "        positive_mask_1 = (input > 0).type_as(grad_output)\n",
        "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
        "        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),\n",
        "                                   torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,\n",
        "                                                 positive_mask_1), positive_mask_2)\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "class GuidedBackpropReLUModel:\n",
        "    def __init__(self, model, use_cuda):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "\n",
        "        def recursive_relu_apply(module_top):\n",
        "            for idx, module in module_top._modules.items():\n",
        "                recursive_relu_apply(module)\n",
        "                if module.__class__.__name__ == 'ReLU':\n",
        "                    module_top._modules[idx] = GuidedBackpropReLU.apply\n",
        "                \n",
        "        # replace ReLU with GuidedBackpropReLU\n",
        "        recursive_relu_apply(self.model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "    def __call__(self, input, index=None):\n",
        "        if self.cuda:\n",
        "            output = self.forward(input.cuda())\n",
        "        else:\n",
        "            output = self.forward(input)\n",
        "\n",
        "        if index == None:\n",
        "            index = np.argmax(output.cpu().data.numpy())\n",
        "\n",
        "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0][index] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        if self.cuda:\n",
        "            one_hot = torch.sum(one_hot.cuda() * output)\n",
        "        else:\n",
        "            one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "        # self.model.features.zero_grad()\n",
        "        # self.model.classifier.zero_grad()\n",
        "        one_hot.backward(retain_graph=True)\n",
        "\n",
        "        output = input.grad.cpu().data.numpy()\n",
        "        output = output[0, :, :, :]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--use-cuda', action='store_true', default=True,\n",
        "                        help='Use NVIDIA GPU acceleration')\n",
        "    # parser.add_argument('--image-path', type=str, default='./examples/both.png',\n",
        "    #                     help='Input image path')\n",
        "    args = parser.parse_args()\n",
        "    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
        "    if args.use_cuda:\n",
        "        print(\"Using GPU for acceleration\")\n",
        "    else:\n",
        "        print(\"Using CPU for computation\")\n",
        "\n",
        "    return args\n",
        "\n",
        "def deprocess_image(img):\n",
        "    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
        "    img = img - np.mean(img)\n",
        "    img = img / (np.std(img) + 1e-5)\n",
        "    img = img * 0.1\n",
        "    img = img + 0.5\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return np.uint8(img*255)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\" python grad_cam.py <path_to_image>\n",
        "    1. Loads an image with opencv.\n",
        "    2. Preprocesses it for VGG19 and converts to a pytorch variable.\n",
        "    3. Makes a forward pass to find the category index with the highest score,\n",
        "    and computes intermediate activations.\n",
        "    Makes the visualization. \"\"\"\n",
        "\n",
        "    args = get_args()\n",
        "\n",
        "    # Can work with any model, but it assumes that the model has a\n",
        "    # feature method, and a classifier method,\n",
        "    # as in the VGG models in torchvision.\n",
        "\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    grad_cam = GradCam(model=model, feature_module=model.layer4, \\\n",
        "                       target_layer_names=[\"2\"], use_cuda=args.use_cuda)\n",
        "\n",
        "    img = cv2.imread(\"/content/gdrive/My Drive/DeepLearning/Face_detection/test/test25.jpg\", 1)\n",
        "    img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "    input = preprocess_image(img)\n",
        "\n",
        "    # If None, returns the map for the highest scoring category.\n",
        "    # Otherwise, targets the requested index.\n",
        "    target_index = None\n",
        "    mask = grad_cam(input, target_index)\n",
        "\n",
        "    show_cam_on_image(img, mask)\n",
        "\n",
        "    gb_model = GuidedBackpropReLUModel(model=model, use_cuda=args.use_cuda)\n",
        "    print(model._modules.items())\n",
        "    gb = gb_model(input, index=target_index)\n",
        "    gb = gb.transpose((1, 2, 0))\n",
        "    cam_mask = cv2.merge([mask, mask, mask])\n",
        "    cam_gb = deprocess_image(cam_mask*gb)\n",
        "    gb = deprocess_image(gb)\n",
        "\n",
        "    cv2.imwrite('gb.jpg', gb)\n",
        "    cv2.imwrite('cam_gb.jpg', cam_gb)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}